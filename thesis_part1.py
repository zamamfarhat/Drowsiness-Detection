# -*- coding: utf-8 -*-
"""Thesis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UAoBuIiMjj0sj_Pb49m89s-_aYY1HJkP

## Importing Libraries and Dataset
"""

# Step 1: Importing the Required Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import classification_report, confusion_matrix
import zipfile
import os
from google.colab import drive
from sklearn.metrics import precision_score, recall_score, f1_score, classification_report
import random
import cv2
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import torchvision.datasets as datasets
from torch.utils.data import DataLoader
import torch.nn.functional as F

!pip install optuna
import optuna

!pip install scikit-fuzzy
import skfuzzy as fuzz

# Step 2: Mount Google Drive and Upload kaggle.json
from google.colab import files

# Upload kaggle.json file (you can manually upload it from your local system)
files.upload()

# Step 3: Make a directory for Kaggle configuration and move the json file there
!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Step 4: Download the Driver Drowsiness Dataset from Kaggle
!kaggle datasets download -d ismailnasri20/driver-drowsiness-dataset-ddd

# Unzip the dataset
!unzip /content/driver-drowsiness-dataset-ddd.zip -d /content/drowsiness_data

!ls /content/drowsiness_data


# Verify if images are present in the Drowsy and Non Drowsy folders
drowsy_dir = os.path.join('/content/drowsiness_data/Driver Drowsiness Dataset (DDD)', 'Drowsy')
non_drowsy_dir = os.path.join('/content/drowsiness_data/Driver Drowsiness Dataset (DDD)', 'Non Drowsy')

print(f"Drowsy images: {len(os.listdir(drowsy_dir))}")
print(f"Non Drowsy images: {len(os.listdir(non_drowsy_dir))}")

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

"""## Train Test Split"""

#Train/Test Split

import os
import shutil
from sklearn.model_selection import train_test_split

# Correct dataset directory
dataset_dir = '/content/drowsiness_data/Driver Drowsiness Dataset (DDD)'

# Ensure directory names match exactly what you found in 'ls' (check the capitalization)
categories = ['Drowsy', 'Non Drowsy']  # Match the exact folder names
all_images = []

# Collect all images and their corresponding labels
for category in categories:
    category_dir = os.path.join(dataset_dir, category)
    if not os.path.exists(category_dir):
        print(f"Directory not found: {category_dir}")
    else:
        images = os.listdir(category_dir)
        all_images.extend([(os.path.join(category_dir, img), category) for img in images])

# Split images into training and test sets, stratified by labels
image_paths, labels = zip(*all_images)  # Unzip into separate lists
train_paths, test_paths, train_labels, test_labels = train_test_split(
    image_paths, labels, test_size=0.2, stratify=labels, random_state=42
)

# Create train and test directories for both categories
train_dir = '/content/drowsiness_data/train'
test_dir = '/content/drowsiness_data/test'
os.makedirs(train_dir, exist_ok=True)
os.makedirs(test_dir, exist_ok=True)

for category in categories:
    os.makedirs(os.path.join(train_dir, category.lower()), exist_ok=True)
    os.makedirs(os.path.join(test_dir, category.lower()), exist_ok=True)

# Move train images
for src in train_paths:
    category = os.path.basename(os.path.dirname(src)).lower()  # Get category from path
    dst = os.path.join(train_dir, category, os.path.basename(src))
    shutil.move(src, dst)

# Move test images
for src in test_paths:
    category = os.path.basename(os.path.dirname(src)).lower()  # Get category from path
    dst = os.path.join(test_dir, category, os.path.basename(src))
    shutil.move(src, dst)

print("Dataset split into train and test sets successfully!")

"""## Data Exploration

### Counting Images, Visualizing Class Distribution, Chekcing and Plotting Class Balance, Analyzing Images and Plotting Image Size Distribution
"""

import os
import matplotlib.pyplot as plt

# Count images in each class for train and test sets
train_drowsy_dir = '/content/drowsiness_data/train/drowsy'
train_non_drowsy_dir = '/content/drowsiness_data/train/non drowsy'
test_drowsy_dir = '/content/drowsiness_data/test/drowsy'
test_non_drowsy_dir = '/content/drowsiness_data/test/non drowsy'

# Function to count images
def count_images(directory):
    return len(os.listdir(directory))

# Counting images
train_drowsy_count = count_images(train_drowsy_dir)
train_non_drowsy_count = count_images(train_non_drowsy_dir)
test_drowsy_count = count_images(test_drowsy_dir)
test_non_drowsy_count = count_images(test_non_drowsy_dir)

# Print exact counts
print(f"Train Drowsy images: {train_drowsy_count}")
print(f"Train Non Drowsy images: {train_non_drowsy_count}")
print(f"Test Drowsy images: {test_drowsy_count}")
print(f"Test Non Drowsy images: {test_non_drowsy_count}")

# Visualize class distribution
labels = ['Train Drowsy', 'Train Non Drowsy', 'Test Drowsy', 'Test Non Drowsy']
counts = [train_drowsy_count, train_non_drowsy_count, test_drowsy_count, test_non_drowsy_count]

plt.figure(figsize=(10, 6))
plt.bar(labels, counts, color=['blue', 'green', 'red', 'purple'])
plt.title('Class Distribution in Train and Test Sets')
plt.xlabel('Category')
plt.ylabel('Number of Images')
plt.show()


# Check class balance
def count_images_in_directory(directory):
    return len(os.listdir(directory))

drowsy_train_count = count_images_in_directory(train_drowsy_dir)
non_drowsy_train_count = count_images_in_directory(train_non_drowsy_dir)

# Display class balance
print(f"Train Drowsy Images: {drowsy_train_count}")
print(f"Train Non-Drowsy Images: {non_drowsy_train_count}")

# Plot class distribution
plt.figure(figsize=(6, 4))
plt.bar(['Train Drowsy', 'Train Non-Drowsy'], [drowsy_train_count, non_drowsy_train_count], color=['blue', 'green'])
plt.title('Class Balance in Train Set')
plt.xlabel('Category')
plt.ylabel('Number of Images')
plt.show()

# Total number of images in train and test sets
total_train_images = train_drowsy_count + train_non_drowsy_count
total_test_images = test_drowsy_count + test_non_drowsy_count
total_images = total_train_images + total_test_images

# Print percentages
print(f"Percentage of Train Drowsy images: {train_drowsy_count / total_images * 100:.2f}%")
print(f"Percentage of Train Non Drowsy images: {train_non_drowsy_count / total_images * 100:.2f}%")
print(f"Percentage of Test Drowsy images: {test_drowsy_count / total_images * 100:.2f}%")
print(f"Percentage of Test Non Drowsy images: {test_non_drowsy_count / total_images * 100:.2f}%")


# Analyze image sizes
def analyze_image_sizes(directory):
    heights = []
    widths = []
    for image_file in os.listdir(directory):
        img_path = os.path.join(directory, image_file)
        img = cv2.imread(img_path)
        if img is not None:  # Check if the image is loaded correctly
            h, w, _ = img.shape
            heights.append(h)
            widths.append(w)
        else:
            print(f"Could not load image: {img_path}")
    return heights, widths

# Analyze image sizes for drowsy and non-drowsy datasets
train_drowsy_heights, train_drowsy_widths = analyze_image_sizes(train_drowsy_dir)
train_non_drowsy_heights, train_non_drowsy_widths = analyze_image_sizes(train_non_drowsy_dir)

# Calculate aspect ratios
train_drowsy_aspect_ratios = np.array(train_drowsy_widths) / np.array(train_drowsy_heights)
train_non_drowsy_aspect_ratios = np.array(train_non_drowsy_widths) / np.array(train_non_drowsy_heights)

# Set the Seaborn style
sns.set(style="whitegrid")

# Plot image size distributions
plt.figure(figsize=(16, 12))

# Train Drowsy Image Size Distribution (Heights)
plt.subplot(3, 2, 1)
sns.histplot(train_drowsy_heights, bins=30, kde=True, color='blue', alpha=0.5)
plt.axvline(np.mean(train_drowsy_heights), color='darkblue', linestyle='--', label='Mean')
plt.title('Train Drowsy Image Heights Distribution')
plt.xlabel('Height (pixels)')
plt.ylabel('Frequency')
plt.legend()

# Train Drowsy Image Size Distribution (Widths)
plt.subplot(3, 2, 2)
sns.histplot(train_drowsy_widths, bins=30, kde=True, color='red', alpha=0.5)
plt.axvline(np.mean(train_drowsy_widths), color='darkred', linestyle='--', label='Mean')
plt.title('Train Drowsy Image Widths Distribution')
plt.xlabel('Width (pixels)')
plt.ylabel('Frequency')
plt.legend()

# Train Non-Drowsy Image Size Distribution (Heights)
plt.subplot(3, 2, 3)
sns.histplot(train_non_drowsy_heights, bins=30, kde=True, color='brown', alpha=0.5)
plt.axvline(np.mean(train_non_drowsy_heights), color='purple', linestyle='--', label='Mean')
plt.title('Train Non-Drowsy Image Heights Distribution')
plt.xlabel('Height (pixels)')
plt.ylabel('Frequency')
plt.legend()

# Train Non-Drowsy Image Size Distribution (Widths)
plt.subplot(3, 2, 4)
sns.histplot(train_non_drowsy_widths, bins=30, kde=True, color='black', alpha=0.5)
plt.axvline(np.mean(train_non_drowsy_widths), color='gray', linestyle='--', label='Mean')
plt.title('Train Non-Drowsy Image Widths Distribution')
plt.xlabel('Width (pixels)')
plt.ylabel('Frequency')
plt.legend()

# Aspect Ratio Distribution
plt.subplot(3, 1, 3)
sns.histplot(train_drowsy_aspect_ratios, bins=30, kde=True, color='purple', alpha=0.5, label='Drowsy')
sns.histplot(train_non_drowsy_aspect_ratios, bins=30, kde=True, color='orange', alpha=0.5, label='Non-Drowsy')
plt.axvline(np.mean(train_drowsy_aspect_ratios), color='darkviolet', linestyle='--', label='Mean Drowsy')
plt.axvline(np.mean(train_non_drowsy_aspect_ratios), color='gold', linestyle='--', label='Mean Non-Drowsy')
plt.title('Aspect Ratio Distribution')
plt.xlabel('Aspect Ratio (Width / Height)')
plt.ylabel('Frequency')
plt.legend()

plt.tight_layout()
plt.show()

"""## Data Cleaning

### Filtering Outliers and Calculating Statistics
"""

import os
import cv2
import numpy as np

# Function to calculate image statistics
def calculate_image_statistics(image_path):
    image = cv2.imread(image_path)
    if image is None:
        return None, None, None  # Handle images that couldn't be read

    # Calculate mean, variance, std deviation across color channels
    mean = np.mean(image, axis=(0, 1))
    variance = np.var(image, axis=(0, 1))
    std_dev = np.std(image, axis=(0, 1))

    return mean, variance, std_dev

# Function to filter outliers
def filter_outliers(mean, threshold_low=10, threshold_high=240):
    return all(threshold_low < m < threshold_high for m in mean)

# Function to compute dataset statistics
def compute_dataset_statistics(directory):
    means, variances, std_devs = [], [], []

    for image_file in os.listdir(directory):
        image_path = os.path.join(directory, image_file)
        statistics = calculate_image_statistics(image_path)

        if statistics is None:  # Skip if the image couldn't be read
            continue

        mean, variance, std_dev = statistics

        if filter_outliers(mean):  # Only include images within the thresholds
            means.append(mean)
            variances.append(variance)
            std_devs.append(std_dev)

    # Aggregate statistics
    overall_mean = np.mean(means, axis=0)
    overall_variance = np.mean(variances, axis=0)
    overall_std_dev = np.mean(std_devs, axis=0)

    return overall_mean, overall_variance, overall_std_dev

# Define directories
train_drowsy_dir = '/content/drowsiness_data/train/drowsy'
train_non_drowsy_dir = '/content/drowsiness_data/train/non drowsy'

# Compute statistics for both datasets
drowsy_statistics = compute_dataset_statistics(train_drowsy_dir)
non_drowsy_statistics = compute_dataset_statistics(train_non_drowsy_dir)

# Print the results for Drowsy and Non-Drowsy
print("Drowsy Images Statistics:")
print(f"Mean: {drowsy_statistics[0]}")
print(f"Variance: {drowsy_statistics[1]}")
print(f"Standard Deviation: {drowsy_statistics[2]}")

print("\nNon-Drowsy Images Statistics:")
print(f"Mean: {non_drowsy_statistics[0]}")
print(f"Variance: {non_drowsy_statistics[1]}")
print(f"Standard Deviation: {non_drowsy_statistics[2]}")

# Now, to calculate overall statistics across both categories
# Combine the statistics from both categories
combined_means = [drowsy_statistics[0], non_drowsy_statistics[0]]
combined_variances = [drowsy_statistics[1], non_drowsy_statistics[1]]
combined_std_devs = [drowsy_statistics[2], non_drowsy_statistics[2]]

# Overall statistics
overall_mean = np.mean(combined_means, axis=0)
overall_variance = np.mean(combined_variances, axis=0)
overall_std_dev = np.mean(combined_std_devs, axis=0)

# Print overall statistics
print("\nOverall Dataset Statistics:")
print(f"Mean: {overall_mean}")
print(f"Variance: {overall_variance}")
print(f"Standard Deviation: {overall_std_dev}")

"""### Checking Image Integrity"""

# Detect corrupted images
def check_image_integrity(directory):
    for img_file in os.listdir(directory):
        img_path = os.path.join(directory, img_file)
        try:
            img = cv2.imread(img_path)
            if img is None:
                print(f"Corrupted image found: {img_path}")
                os.remove(img_path)  # Remove corrupted image
        except Exception as e:
            print(f"Error loading image: {img_path} - {e}")

check_image_integrity('/content/drowsiness_data/train/drowsy')
check_image_integrity('/content/drowsiness_data/train/non drowsy')

"""### Transformation and Normalization"""

# Using the calculated means and standard deviations
mean = [86.18327214, 97.50867414, 128.097746]
std = [48.52839612, 52.72049091, 58.92662116]

# Define transformations with normalization
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.ToTensor(),
    transforms.Normalize(mean=[m / 255.0 for m in mean], std=[s / 255.0 for s in std])
])

train_data = datasets.ImageFolder(root='/content/drowsiness_data/train', transform=transform)
test_data = datasets.ImageFolder(root='/content/drowsiness_data/test', transform=transform)

train_loader = DataLoader(train_data, batch_size=64, shuffle=True)
test_loader = DataLoader(test_data, batch_size=64, shuffle=False)

"""### Stratified Shuffle and Creating Train and Val sets"""

from sklearn.model_selection import StratifiedShuffleSplit

# Stratified shuffle split
sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
labels = [label for _, label in train_data]

for train_index, test_index in sss.split(np.zeros(len(labels)), labels):
    train_sampler = torch.utils.data.SubsetRandomSampler(train_index)
    val_sampler = torch.utils.data.SubsetRandomSampler(test_index)

train_loader_stratified = DataLoader(train_data, batch_size=64, sampler=train_sampler)
val_loader_stratified = DataLoader(train_data, batch_size=64, sampler=val_sampler)

"""#### Printing Class Weights"""

# Define device for PyTorch (use GPU if available, otherwise CPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Calculate class weights
from sklearn.utils.class_weight import compute_class_weight

# Get all labels from the dataset
all_labels = [label for _, label in train_data]
class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(all_labels), y=all_labels)
class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)

# Define the loss function with weights
criterion = nn.CrossEntropyLoss(weight=class_weights)

print(f"Class weights: {class_weights}")
print(f"Criterion: {criterion}")

"""## Data Preprocessing

### Augmentation
"""

import albumentations as A
from albumentations.pytorch import ToTensorV2
import cv2
import numpy as np
import torch
from torch.utils.data import DataLoader, SubsetRandomSampler
from torchvision import datasets
from sklearn.model_selection import StratifiedShuffleSplit
import matplotlib.pyplot as plt

# Dataset-specific normalization values
mean = [86.20332036 / 255.0, 97.57274173 / 255.0, 128.23810727 / 255.0]
std = [48.52471532 / 255.0, 52.71026118 / 255.0, 58.87592237 / 255.0]

# Define augmentation pipeline for training
train_augmentations = A.Compose([
    A.Resize(224, 224),
    A.HorizontalFlip(p=0.5),
    A.RandomBrightnessContrast(p=0.5),
    A.Rotate(limit=15),
    A.Normalize(mean=mean, std=std),
    ToTensorV2()
])

# Basic preprocessing (resize & normalize) for validation
val_transform = A.Compose([
    A.Resize(224, 224),
    A.Normalize(mean=mean, std=std),
    ToTensorV2()
])

# Custom dataset class to integrate albumentations with ImageFolder
class AlbumentationsDataset(torch.utils.data.Dataset):
    def __init__(self, dataset, transform=None):
        self.dataset = dataset
        self.transform = transform

    def __getitem__(self, idx):
        image, label = self.dataset[idx]
        image = np.array(image)  # Convert PIL image to numpy array for albumentations

        if self.transform:
            image = self.transform(image=image)['image']

        return image, label

    def __len__(self):
        return len(self.dataset)

# Load data using ImageFolder and custom transformations
train_data = datasets.ImageFolder(root='/content/drowsiness_data/train')
val_data = datasets.ImageFolder(root='/content/drowsiness_data/train')  # Same as train; we’ll use samplers to split

# Wrap datasets with Albumentations
train_dataset = AlbumentationsDataset(train_data, transform=train_augmentations)
val_dataset = AlbumentationsDataset(val_data, transform=val_transform)

# Stratified Split for train-validation split
sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
labels = [label for _, label in train_data]

for train_index, val_index in sss.split(np.zeros(len(labels)), labels):
    train_sampler = SubsetRandomSampler(train_index)
    val_sampler = SubsetRandomSampler(val_index)

# DataLoaders with samplers
train_loader = DataLoader(train_dataset, batch_size=64, sampler=train_sampler)
val_loader = DataLoader(val_dataset, batch_size=64, sampler=val_sampler)

# Display sample augmented image
def show_augmented_image():
    sample_image, _ = train_data[0]  # Get an image from the dataset
    sample_image = np.array(sample_image)  # Convert to numpy for albumentations
    augmented = train_augmentations(image=sample_image)['image']  # Apply augmentations

    plt.subplot(1, 2, 1)
    plt.imshow(sample_image)
    plt.title("Original Image")
    plt.subplot(1, 2, 2)
    plt.imshow(augmented.permute(1, 2, 0).numpy())
    plt.title("Augmented Image")
    plt.show()

show_augmented_image()

"""#### Augmentation on a Sample Image"""

# Load and show an augmented image from train_loader
data_iter = iter(train_loader)
sample_image, _ = next(data_iter)  # Get the first batch
sample_image = sample_image[0]  # Select the first image in the batch

# Apply augmentation to the sample image (example purposes)
augmented = train_augmentations(image=sample_image.permute(1, 2, 0).numpy())['image']

# Display original and augmented images
plt.subplot(1, 2, 1)
plt.imshow(sample_image.permute(1, 2, 0).numpy())  # Convert to HxWxC for matplotlib
plt.title("Original Image")
plt.subplot(1, 2, 2)
plt.imshow(augmented.permute(1, 2, 0).numpy())  # Convert back to HxWxC
plt.title("Augmented Image")
plt.show()

"""### Analyzing Brightness and Contrast"""

# Analyze brightness and contrast using the stratified DataLoader
def analyze_brightness_contrast(data_loader):
    brightness_values, contrast_values = [], []
    for images, _ in data_loader:
        for img in images:
            img_gray = cv2.cvtColor(img.permute(1, 2, 0).numpy(), cv2.COLOR_RGB2GRAY)
            brightness_values.append(np.mean(img_gray))
            contrast_values.append(np.std(img_gray))
    return brightness_values, contrast_values

# Analyze brightness/contrast for train_loader images
brightness_train, contrast_train = analyze_brightness_contrast(train_loader)
brightness_val, contrast_val = analyze_brightness_contrast(val_loader)

# Plot results
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.hist(brightness_train, bins=30, alpha=0.5, label='Training')
plt.hist(brightness_val, bins=30, alpha=0.5, label='Validation')
plt.title('Brightness Distribution')
plt.xlabel('Brightness')
plt.ylabel('Frequency')
plt.legend()

plt.subplot(1, 2, 2)
plt.hist(contrast_train, bins=30, alpha=0.5, label='Training')
plt.hist(contrast_val, bins=30, alpha=0.5, label='Validation')
plt.title('Contrast Distribution')
plt.xlabel('Contrast')
plt.ylabel('Frequency')
plt.legend()
plt.show()

"""### Structural similarity index measure (SSIM)"""

from skimage.metrics import structural_similarity as ssim

# Image similarity check (e.g., within one batch in train_loader)
data_iter = iter(train_loader)
sample_batch, _ = next(data_iter)
img1 = sample_batch[0].permute(1, 2, 0).numpy()  # Convert to HxWxC format
img2 = sample_batch[1].permute(1, 2, 0).numpy()  # Convert to HxWxC format

# Calculate SSIM with smaller window size, specified channel axis, and data range
similarity_index, _ = ssim(img1, img2, win_size=3, full=True, channel_axis=2, data_range=1.0)
print(f'Structural Similarity Index: {similarity_index}')

"""### Train and Validation Set"""

# Use Stratified DataLoader for training and validation
train_loader = train_loader_stratified
val_loader = val_loader_stratified

"""## Inception Model: Training and Testing"""

from tensorflow.keras.applications import InceptionV3
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import precision_recall_fscore_support, accuracy_score

import os
import shutil
from sklearn.model_selection import train_test_split

# Directories
train_dir = '/content/drowsiness_data/train'
val_dir = '/content/drowsiness_data/validation'

# Ensure validation directory exists
os.makedirs(val_dir, exist_ok=True)
os.makedirs(os.path.join(val_dir, 'drowsy'), exist_ok=True)
os.makedirs(os.path.join(val_dir, 'non drowsy'), exist_ok=True)

# Get all image paths and labels
categories = ['drowsy', 'non drowsy']
all_images = []
labels = []

for category in categories:
    category_dir = os.path.join(train_dir, category)
    for img_file in os.listdir(category_dir):
        all_images.append(os.path.join(category_dir, img_file))
        labels.append(category)

# Stratified split
train_images, val_images, train_labels, val_labels = train_test_split(
    all_images, labels, test_size=0.2, stratify=labels, random_state=42
)

# Move validation images to validation directory
for img_path, label in zip(val_images, val_labels):
    val_category_dir = os.path.join(val_dir, label)
    shutil.move(img_path, os.path.join(val_category_dir, os.path.basename(img_path)))

print("Validation set created successfully!")

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Image data generators
train_datagen = ImageDataGenerator(
    rescale=1.0/255.0,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode="nearest"
)

val_datagen = ImageDataGenerator(rescale=1.0/255.0)
test_datagen = ImageDataGenerator(rescale=1.0/255.0)

# Flow from directory
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary'
)

val_generator = val_datagen.flow_from_directory(
    val_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary'
)

test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary',
    shuffle=False
)

"""### Building inception model"""

# Load pre-trained InceptionV3 model without the top layers
base_model = InceptionV3(weights="imagenet", include_top=False, input_shape=(224, 224, 3))

# Freeze base model layers
base_model.trainable = False

# Add custom layers
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(1024, activation="relu")(x)
x = Dropout(0.5)(x)
predictions = Dense(1, activation="sigmoid")(x)

# Create model
model = Model(inputs=base_model.input, outputs=predictions)

# Compile the model
model.compile(optimizer=Adam(learning_rate=1e-4), loss="binary_crossentropy", metrics=["accuracy"])

"""### Call backs for early stopping"""

# Callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Option 1: Using .keras extension (preferred)
checkpoint = ModelCheckpoint("inceptionv3_best_model.keras", save_best_only=True, monitor='val_loss')

# Option 2: Using .h5 extension explicitly
# checkpoint = ModelCheckpoint("inceptionv3_best_model.h5", save_best_only=True, monitor='val_loss', save_format="h5")

callbacks = [early_stopping, checkpoint]

"""### Training Model"""

history = model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=50,
    callbacks=callbacks
)

# Load the best model
model.load_weights("inceptionv3_best_model.keras")

# Evaluate on test data
test_loss, test_accuracy = model.evaluate(test_generator)
y_true = test_generator.classes
y_pred = (model.predict(test_generator) > 0.5).astype(int).flatten()

# Calculate Precision, Recall, and F1-score
precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')
accuracy = accuracy_score(y_true, y_pred)

print(f"Test Loss: {test_loss}")
print(f"Test Accuracy: {test_accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")

# Plot accuracy and loss
plt.figure(figsize=(12, 6))

# Accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Model Accuracy')
plt.legend()

# Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Model Loss')
plt.legend()

plt.tight_layout()
plt.show()

"""## Inception with FIS and NAS"""

import random
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix
import seaborn as sns
from tensorflow.keras.applications import InceptionV3
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Fuzzy logic inference function
def fuzzy_inference(output):
    x_drowsiness = np.arange(0, 1.01, 0.01)
    drowsy_mf = fuzz.trapmf(x_drowsiness, [0.0, 0.0, 0.05, 0.15])
    moderately_drowsy_mf = fuzz.trimf(x_drowsiness, [0.10, 0.25, 0.40])
    slightly_drowsy_mf = fuzz.trimf(x_drowsiness, [0.15, 0.30, 0.50])
    non_drowsy_mf = fuzz.trapmf(x_drowsiness, [0.30, 0.50, 1.0, 1.0])

    drowsy_activation = fuzz.interp_membership(x_drowsiness, drowsy_mf, output)
    moderately_drowsy_activation = fuzz.interp_membership(x_drowsiness, moderately_drowsy_mf, output)
    slightly_drowsy_activation = fuzz.interp_membership(x_drowsiness, slightly_drowsy_mf, output)
    non_drowsy_activation = fuzz.interp_membership(x_drowsiness, non_drowsy_mf, output)

    final_decision = (drowsy_activation * 0.75 +
                      moderately_drowsy_activation * 0.5 +
                      slightly_drowsy_activation * 0.25 +
                      non_drowsy_activation * 0.0)
    return np.clip(final_decision, 0, 1)

# Neural Architecture Search (NAS) for InceptionV3
def inception_nas(num_trials, train_generator, val_generator, input_shape=(224, 224, 3)):
    best_model = None
    best_metrics = None
    best_val_accuracy = 0

    for trial in range(num_trials):
        num_dense_units = random.choice([256, 512, 1024])
        dropout_rate = random.uniform(0.4, 0.6)

        # Build InceptionV3 model
        base_model = InceptionV3(weights="imagenet", include_top=False, input_shape=input_shape)
        base_model.trainable = False

        x = base_model.output
        x = GlobalAveragePooling2D()(x)
        x = Dense(num_dense_units, activation="relu")(x)
        x = Dropout(dropout_rate)(x)
        predictions = Dense(1, activation="sigmoid")(x)

        model = Model(inputs=base_model.input, outputs=predictions)
        model.compile(optimizer=Adam(learning_rate=1e-4), loss="binary_crossentropy", metrics=["accuracy"])

        # Callbacks
        early_stopping = EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True)
        checkpoint = ModelCheckpoint(f"inceptionv3_trial_{trial}.keras", save_best_only=True, monitor="val_loss")

        # Train the model
        history = model.fit(
            train_generator,
            validation_data=val_generator,
            epochs=10,
            callbacks=[early_stopping, checkpoint],
            verbose=1
        )

        # Check validation accuracy
        val_accuracy = history.history["val_accuracy"][-1]
        if val_accuracy > best_val_accuracy:
            best_val_accuracy = val_accuracy
            best_model = model
            best_metrics = history

    return best_model, best_metrics

# Train the model with NAS
best_model, best_metrics = inception_nas(
    num_trials=1,  # Try different architectures
    train_generator=train_generator,
    val_generator=val_generator
)

# Evaluate and test the model
best_model.load_weights("inceptionv3_trial_0.keras")  # Load the best weights saved
test_loss, test_accuracy = best_model.evaluate(test_generator)
y_true = test_generator.classes
y_pred = (best_model.predict(test_generator) > 0.5).astype(int).flatten()

# Calculate metrics
precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average="binary")
accuracy = accuracy_score(y_true, y_pred)

print(f"Test Loss: {test_loss}")
print(f"Test Accuracy: {test_accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")

# Apply fuzzy inference
fuzzy_outputs = np.array([fuzzy_inference(pred) for pred in best_model.predict(test_generator)])
print(f"Fuzzy Adjusted Outputs: {fuzzy_outputs[:10]}")  # Example fuzzy results

# Confusion Matrix
cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Non Drowsy", "Drowsy"], yticklabels=["Non Drowsy", "Drowsy"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

# Plot Training Metrics
def plot_training_metrics(history):
    plt.figure(figsize=(12, 6))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history["accuracy"], label="Train Accuracy")
    plt.plot(history.history["val_accuracy"], label="Val Accuracy")
    plt.xlabel("Epochs")
    plt.ylabel("Accuracy")
    plt.title("Model Accuracy")
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history["loss"], label="Train Loss")
    plt.plot(history.history["val_loss"], label="Val Loss")
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.title("Model Loss")
    plt.legend()

    plt.tight_layout()
    plt.show()

# Visualize training metrics
plot_training_metrics(best_metrics)

"""## DENSENET IMPLEMENTATION"""

from tensorflow.keras.applications import DenseNet121
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.metrics import Precision, Recall
import matplotlib.pyplot as plt

# Load DenseNet121 without top layers
base_model = DenseNet121(weights="imagenet", include_top=False, input_shape=(224, 224, 3))
base_model.trainable = False

# Add custom layers
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(512, activation='relu')(x)
x = Dropout(0.5)(x)
predictions = Dense(1, activation='sigmoid')(x)

# Create model
densenet_model = Model(inputs=base_model.input, outputs=predictions)

# Compile the model
densenet_model.compile(optimizer=Adam(learning_rate=1e-4),
                       loss='binary_crossentropy',
                       metrics=['accuracy', Precision(name='precision'), Recall(name='recall')])

# Callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
checkpoint = ModelCheckpoint("densenet_best.keras", save_best_only=True, monitor='val_loss')

# Train
history = densenet_model.fit(train_generator,
                             validation_data=val_generator,
                             epochs=10,
                             callbacks=[early_stopping, checkpoint])

# Calculate F1 scores for every epoch
f1_scores_train = []
f1_scores_val = []

for epoch in range(len(history.history['precision'])):
    train_precision = history.history['precision'][epoch]
    train_recall = history.history['recall'][epoch]
    val_precision = history.history['val_precision'][epoch]
    val_recall = history.history['val_recall'][epoch]

    # F1 score for training
    train_f1 = (2 * train_precision * train_recall) / (train_precision + train_recall + 1e-7)
    f1_scores_train.append(train_f1)

    # F1 score for validation
    val_f1 = (2 * val_precision * val_recall) / (val_precision + val_recall + 1e-7)
    f1_scores_val.append(val_f1)

# Plot metrics separately
plt.figure(figsize=(14, 10))

# Accuracy
plt.subplot(2, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy', marker='o')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='o')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Accuracy Curve')
plt.legend()
plt.grid()

# Precision
plt.subplot(2, 2, 2)
plt.plot(history.history['precision'], label='Train Precision', marker='o')
plt.plot(history.history['val_precision'], label='Validation Precision', marker='o')
plt.xlabel('Epochs')
plt.ylabel('Precision')
plt.title('Precision Curve')
plt.legend()
plt.grid()

# Recall
plt.subplot(2, 2, 3)
plt.plot(history.history['recall'], label='Train Recall', marker='o')
plt.plot(history.history['val_recall'], label='Validation Recall', marker='o')
plt.xlabel('Epochs')
plt.ylabel('Recall')
plt.title('Recall Curve')
plt.legend()
plt.grid()

# F1-Score
plt.subplot(2, 2, 4)
plt.plot(f1_scores_train, label='Train F1-Score', marker='o')
plt.plot(f1_scores_val, label='Validation F1-Score', marker='o')
plt.xlabel('Epochs')
plt.ylabel('F1-Score')
plt.title('F1-Score Curve')
plt.legend()
plt.grid()

# Adjust layout
plt.tight_layout()
plt.show()

"""## Densenet with FIS and NAS"""

import random
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.applications import DenseNet121
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix

# Fuzzy logic inference function
def fuzzy_inference(output):
    x_drowsiness = np.arange(0, 1.01, 0.01)
    drowsy_mf = fuzz.trapmf(x_drowsiness, [0.0, 0.0, 0.05, 0.15])
    moderately_drowsy_mf = fuzz.trimf(x_drowsiness, [0.10, 0.25, 0.40])
    slightly_drowsy_mf = fuzz.trimf(x_drowsiness, [0.15, 0.30, 0.50])
    non_drowsy_mf = fuzz.trapmf(x_drowsiness, [0.30, 0.50, 1.0, 1.0])

    drowsy_activation = fuzz.interp_membership(x_drowsiness, drowsy_mf, output)
    moderately_drowsy_activation = fuzz.interp_membership(x_drowsiness, moderately_drowsy_mf, output)
    slightly_drowsy_activation = fuzz.interp_membership(x_drowsiness, slightly_drowsy_mf, output)
    non_drowsy_activation = fuzz.interp_membership(x_drowsiness, non_drowsy_mf, output)

    final_decision = (drowsy_activation * 0.75 +
                      moderately_drowsy_activation * 0.5 +
                      slightly_drowsy_activation * 0.25 +
                      non_drowsy_activation * 0.0)
    return np.clip(final_decision, 0, 1)

# Neural Architecture Search (NAS) for DenseNet
def densenet_nas(num_trials, train_generator, val_generator, input_shape=(224, 224, 3)):
    best_model = None
    best_metrics = None
    best_val_accuracy = 0

    for trial in range(num_trials):
        num_dense_units = random.choice([256, 512, 1024])
        dropout_rate = random.uniform(0.4, 0.6)

        # Build DenseNet model
        base_model = DenseNet121(weights="imagenet", include_top=False, input_shape=input_shape)
        base_model.trainable = False

        x = base_model.output
        x = GlobalAveragePooling2D()(x)
        x = Dense(num_dense_units, activation="relu")(x)
        x = Dropout(dropout_rate)(x)
        predictions = Dense(1, activation="sigmoid")(x)

        model = Model(inputs=base_model.input, outputs=predictions)
        model.compile(optimizer=Adam(learning_rate=1e-4), loss="binary_crossentropy", metrics=["accuracy"])

        # Callbacks
        early_stopping = EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True)
        checkpoint = ModelCheckpoint(f"densenet_trial_{trial}.keras", save_best_only=True, monitor="val_loss")

        # Train the model
        history = model.fit(
            train_generator,
            validation_data=val_generator,
            epochs=10,
            callbacks=[early_stopping, checkpoint],
            verbose=1
        )

        # Check validation accuracy
        val_accuracy = history.history["val_accuracy"][-1]
        if val_accuracy > best_val_accuracy:
            best_val_accuracy = val_accuracy
            best_model = model
            best_metrics = history

    return best_model, best_metrics

# Train the model with NAS
best_model, best_metrics = densenet_nas(
    num_trials=1,  # Try different architectures
    train_generator=train_generator,
    val_generator=val_generator
)

# Evaluate and test the model
best_model.load_weights("densenet_trial_0.keras")  # Load the best weights saved
test_loss, test_accuracy = best_model.evaluate(test_generator)
y_true = test_generator.classes
y_pred = (best_model.predict(test_generator) > 0.5).astype(int).flatten()

# Calculate metrics
precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average="binary")
accuracy = accuracy_score(y_true, y_pred)

print(f"Test Loss: {test_loss}")
print(f"Test Accuracy: {test_accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")

# Apply fuzzy inference
fuzzy_outputs = np.array([fuzzy_inference(pred) for pred in best_model.predict(test_generator)])
print(f"Fuzzy Adjusted Outputs: {fuzzy_outputs[:10]}")  # Example fuzzy results

# Confusion Matrix
cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Non Drowsy", "Drowsy"], yticklabels=["Non Drowsy", "Drowsy"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

# Plot Training Metrics
def plot_training_metrics(history):
    plt.figure(figsize=(12, 6))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history["accuracy"], label="Train Accuracy")
    plt.plot(history.history["val_accuracy"], label="Val Accuracy")
    plt.xlabel("Epochs")
    plt.ylabel("Accuracy")
    plt.title("Model Accuracy")
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history["loss"], label="Train Loss")
    plt.plot(history.history["val_loss"], label="Val Loss")
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.title("Model Loss")
    plt.legend()

    plt.tight_layout()
    plt.show()

# Visualize training metrics
plot_training_metrics(best_metrics)

"""## Baseline CNN Model"""

class BaselineCNN(nn.Module):
    def __init__(self, num_classes=2, dropout_rate=0.2):  # Reduced dropout rate
        super(BaselineCNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)  # New layer
        self.bn3 = nn.BatchNorm2d(128)  # BatchNorm for new layer
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(128 * 28 * 28, 512)  # Adjusted input size for added layer
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, num_classes)
        self.dropout = nn.Dropout(dropout_rate)

    def forward(self, x):
        x = self.pool(F.relu(self.bn1(self.conv1(x))))
        x = self.pool(F.relu(self.bn2(self.conv2(x))))
        x = self.pool(F.relu(self.bn3(self.conv3(x))))  # Forward through new layer
        x = x.view(-1, 128 * 28 * 28)  # Adjusted input size for flattening
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.fc3(x)
        return x

"""### Training SetUp"""

import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = BaselineCNN().to(device)

# Loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)

# Learning rate scheduler
scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3)

"""### Training Loop and Validation Loop with Early Stopping"""

from sklearn.metrics import precision_score, recall_score, f1_score
import copy

# Early stopping setup
num_epochs = 50  # Consider increasing this number
best_val_loss = float('inf')
best_model_wts = copy.deepcopy(model.state_dict())
early_stop_counter = 0
patience = 5

# Lists to store training and validation metrics
train_losses, val_losses = [], []
train_accuracies, val_accuracies = [], []
val_precisions, val_recalls, val_f1s = [], [], []  # Store validation metrics

for epoch in range(num_epochs):
    # Training Loop
    model.train()
    running_loss = 0.0
    correct, total = 0, 0

    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()

        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, preds = torch.max(outputs, 1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)

    train_loss = running_loss / len(train_loader)
    train_accuracy = correct / total
    train_losses.append(train_loss)
    train_accuracies.append(train_accuracy)

    # Validation Loop
    model.eval()
    val_loss = 0  # Initialize validation loss
    correct, total = 0, 0  # Initialize correct predictions and total samples
    all_labels, all_preds = [], []  # To store labels and predictions for metrics

    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)

            val_loss += loss.item()  # Accumulate validation loss
            _, preds = torch.max(outputs, 1)
            correct += (preds == labels).sum().item()  # Count correct predictions
            total += labels.size(0)  # Count total samples

            # Extend lists for metric calculation after loop
            all_labels.extend(labels.cpu().numpy())
            all_preds.extend(preds.cpu().numpy())

    val_loss /= len(val_loader)  # Average validation loss
    val_accuracy = correct / total  # Accuracy as the ratio of correct predictions to total
    val_losses.append(val_loss)
    val_accuracies.append(val_accuracy)

    # Calculate validation metrics
    val_precision = precision_score(all_labels, all_preds, average='binary', zero_division=0)
    val_recall = recall_score(all_labels, all_preds, average='binary', zero_division=0)
    val_f1 = f1_score(all_labels, all_preds, average='binary')

    val_precisions.append(val_precision)
    val_recalls.append(val_recall)
    val_f1s.append(val_f1)

    # Adjust the learning rate based on validation loss
    scheduler.step(val_loss)

    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val Precision: {val_precision:.4f}, Val Recall: {val_recall:.4f}, Val F1: {val_f1:.4f}')

    # Early stopping condition
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        best_model_wts = copy.deepcopy(model.state_dict())
        early_stop_counter = 0
    else:
        early_stop_counter += 1

    if early_stop_counter >= patience:
        print('Early stopping!')
        break

# Load the best model weights after training
model.load_state_dict(best_model_wts)

# Save the trained model
torch.save(model.state_dict(), 'baseline_cnn_model.pth')

"""### Testing"""

# Create test dataset and loader
test_dataset = datasets.ImageFolder(root='/content/drowsiness_data/test', transform=transform)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Evaluation loop
model.eval()
test_loss, correct, total = 0, 0, 0
all_labels, all_preds = [], []

with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        loss = criterion(outputs, labels)

        test_loss += loss.item()
        _, preds = torch.max(outputs, 1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)

        all_labels.extend(labels.cpu().numpy())
        all_preds.extend(preds.cpu().numpy())

# Calculate metrics
test_loss /= len(test_loader)  # Average test loss
test_accuracy = correct / total  # Accuracy as the ratio of correct predictions to total

# Calculate precision, recall, and F1-score
precision = precision_score(all_labels, all_preds, average='binary', zero_division=0)
recall = recall_score(all_labels, all_preds, average='binary', zero_division=0)
f1 = f1_score(all_labels, all_preds, average='binary')

# Print results
print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')
print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}')

"""### Hyperparameter Tuning"""

from sklearn.model_selection import ParameterGrid

# Define the hyperparameters to tune
param_grid = {
    'learning_rate': [0.001, 0.01, 0.1],
    'batch_size': [16, 32, 64],
    'dropout_rate': [0.2, 0.5],
}

# Create parameter combinations
grid = list(ParameterGrid(param_grid))

best_val_loss = float('inf')
best_params = {}

# Loop through each combination of parameters
for params in grid:
    print(f"Training with: {params}")

    # Update model and optimizer based on the current set of hyperparameters
    model = BaselineCNN(dropout_rate=params['dropout_rate']).to(device)  # Assuming dropout added in BaselineCNN architecture
    optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'])

    # Update DataLoader with current batch size
    train_loader = DataLoader(train_loader, batch_size=params['batch_size'], shuffle=True)
    val_loader = DataLoader(val_loader, batch_size=params['batch_size'], shuffle=False)

    # Training loop (similar to before)
    for epoch in range(num_epochs):
        # Your training and validation code here...
        # Compare validation loss to keep track of the best model

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_params = params
            best_model_wts = copy.deepcopy(model.state_dict())

print(f"Best Parameters: {best_params}")
model.load_state_dict(best_model_wts)

"""### Plotting Loss and Accuracy"""

import matplotlib.pyplot as plt

# Plot training and validation loss
plt.figure(figsize=(10, 5))
plt.plot(train_losses, label='Training Loss')
plt.plot(val_losses, label='Validation Loss')
plt.title('Loss over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Plot training and validation accuracy
plt.figure(figsize=(10, 5))
plt.plot(train_accuracies, label='Training Accuracy')
plt.plot(val_accuracies, label='Validation Accuracy')
plt.title('Accuracy over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""### Plotting Recall, Precision and F1"""

import matplotlib.pyplot as plt

# Plot Precision, Recall, and F1-Score over epochs
epochs = range(1, len(val_precisions) + 1)

plt.figure(figsize=(12, 4))

# Precision plot
plt.subplot(1, 3, 1)
plt.plot(epochs, val_precisions, label='Validation Precision')
plt.xlabel('Epoch')
plt.ylabel('Precision')
plt.title('Precision over Epochs')
plt.legend()

# Recall plot
plt.subplot(1, 3, 2)
plt.plot(epochs, val_recalls, label='Validation Recall')
plt.xlabel('Epoch')
plt.ylabel('Recall')
plt.title('Recall over Epochs')
plt.legend()

# F1-Score plot
plt.subplot(1, 3, 3)
plt.plot(epochs, val_f1s, label='Validation F1 Score')
plt.xlabel('Epoch')
plt.ylabel('F1 Score')
plt.title('F1 Score over Epochs')
plt.legend()

plt.tight_layout()
plt.show()

"""### Plotting Confusion Matrix"""

from sklearn.metrics import confusion_matrix
import seaborn as sns

# Generate confusion matrix
conf_mat = confusion_matrix(all_labels, all_preds)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""## CNN Model with Fuzzy Logic and NAS

"""

class CNNModel(nn.Module):
    def __init__(self, num_conv_layers=2, num_filters=32, input_size=224):
        super(CNNModel, self).__init__()
        layers = []
        in_channels = 3  # Input has 3 channels (RGB)

        for _ in range(num_conv_layers):
            layers.append(nn.Conv2d(in_channels, num_filters, kernel_size=3, stride=1, padding=1))
            layers.append(nn.BatchNorm2d(num_filters))
            layers.append(nn.ReLU())
            layers.append(nn.MaxPool2d(kernel_size=2, stride=2))
            layers.append(nn.Dropout(0.1))  # Reduced dropout rate
            in_channels = num_filters
            num_filters *= 2

        self.conv_layers = nn.Sequential(*layers)
        dummy_input = torch.randn(1, 3, input_size, input_size)
        with torch.no_grad():
            dummy_output = self.conv_layers(dummy_input)
        self.flatten_size = dummy_output.view(1, -1).size(1)

        self.fc1 = nn.Linear(self.flatten_size, 128)
        self.fc2 = nn.Linear(128, 64)  # Added additional fully connected layer
        self.fc3 = nn.Linear(64, 2)  # For binary classification

    def forward(self, x):
        x = self.conv_layers(x)
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))  # Added activation for the second fully connected layer
        x = self.fc3(x)
        return x

"""### Training Set up"""

# Device setup
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = CNNModel().to(device)

# Loss and optimizer setup
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)

"""### Evaluation Mode"""

from sklearn.metrics import precision_score, recall_score, f1_score

def evaluate_model(model, data_loader):
    model.eval()  # Set the model to evaluation mode
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for images, labels in data_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    precision = precision_score(all_labels, all_preds, average='weighted')
    recall = recall_score(all_labels, all_preds, average='weighted')
    f1 = f1_score(all_labels, all_preds, average='weighted')

    # Calculate accuracy
    accuracy = (np.array(all_preds) == np.array(all_labels)).mean() * 100

    return accuracy, precision, recall, f1

"""### Fuzzy Logic"""

def fuzzy_inference(cnn_output):
    x_drowsiness = np.arange(0, 1.01, 0.01)
    drowsy_mf = fuzz.trapmf(x_drowsiness, [0.0, 0.0, 0.05, 0.15])
    moderately_drowsy_mf = fuzz.trimf(x_drowsiness, [0.10, 0.25, 0.40])
    slightly_drowsy_mf = fuzz.trimf(x_drowsiness, [0.15, 0.30, 0.50])
    non_drowsy_mf = fuzz.trapmf(x_drowsiness, [0.30, 0.50, 1.0, 1.0])

    drowsy_activation = fuzz.interp_membership(x_drowsiness, drowsy_mf, cnn_output)
    moderately_drowsy_activation = fuzz.interp_membership(x_drowsiness, moderately_drowsy_mf, cnn_output)
    slightly_drowsy_activation = fuzz.interp_membership(x_drowsiness, slightly_drowsy_mf, cnn_output)
    non_drowsy_activation = fuzz.interp_membership(x_drowsiness, non_drowsy_mf, cnn_output)

    final_decision = (drowsy_activation * 0.75 +
                      moderately_drowsy_activation * 0.5 +
                      slightly_drowsy_activation * 0.25 +
                      non_drowsy_activation * 0.0)
    return np.clip(final_decision, 0, 1)

import torch
print(torch.cuda.is_available())

"""### NAS"""

def neural_architecture_search(num_trials):
    best_model = None
    best_accuracy = 0

    for trial in range(num_trials):
        num_conv_layers = random.randint(2, 4)
        num_filters = 32 * (2 ** random.randint(0, 3))
        model = CNNModel(num_conv_layers=num_conv_layers, num_filters=num_filters).to(device)
        optimizer = optim.Adam(model.parameters(), lr=0.001)

        for epoch in range(3):
            model.train()
            running_loss, correct, total = 0.0, 0, 0
            scaler = torch.cuda.amp.GradScaler()  # Initialize GradScaler for mixed precision

            for images, labels in train_loader:
                images, labels = images.to(device), labels.to(device)
                optimizer.zero_grad()

                with torch.cuda.amp.autocast():  # Mixed precision context
                    outputs = model(images)
                    loss = criterion(outputs, labels)

                scaler.scale(loss).backward()  # Scale loss
                scaler.step(optimizer)  # Step optimizer
                scaler.update()  # Update scaler

                running_loss += loss.item()
                total += labels.size(0)
                _, predicted = torch.max(outputs.data, 1)
                correct += (predicted == labels).sum().item()

        val_accuracy, val_precision, val_recall, val_f1, _, _ = evaluate_model(model, val_loader)
        if val_accuracy > best_accuracy:
            best_accuracy = val_accuracy
            best_model = model

    return best_model

torch.cuda.empty_cache()

"""### Hyperparameter Tuning for Fuzzy Logic and NAS"""

# Use Stratified DataLoader for training and validation
train_loader = train_loader_stratified
val_loader = val_loader_stratified

# Define the objective function for Optuna
def objective(trial):
    num_conv_layers = trial.suggest_int('num_conv_layers', 2, 4)
    num_filters = trial.suggest_int('num_filters', 32, 128)
    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)

    model = CNNModel(num_conv_layers=num_conv_layers, num_filters=num_filters).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    scaler = torch.amp.GradScaler()  # Updated to match the correct usage

    # Training Loop
    for epoch in range(3):  # You can adjust the number of epochs
        model.train()
        running_loss, correct, total = 0.0, 0, 0

        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()

            with torch.amp.autocast(device_type='cuda'):
                outputs = model(images)
                loss = criterion(outputs, labels)

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            running_loss += loss.item()
            total += labels.size(0)
            _, predicted = torch.max(outputs.data, 1)
            correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    all_labels, all_preds = [], []
    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            all_labels.extend(labels.cpu().numpy())
            all_preds.extend(predicted.cpu().numpy())

    # Calculate metrics for validation
    val_accuracy = accuracy_score(all_labels, all_preds)
    return val_accuracy

# Run Optuna optimization
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=10)

print("Best hyperparameters:", study.best_params)

"""### Training Loop and Validation Loop"""

# Initialize lists to store metrics
val_accuracies = []
val_precisions = []
val_recalls = []
val_f1s = []

# Training loop with early stopping
checkpoint_path = "best_model.pth"
patience, best_val_loss, epochs_without_improvement = 5, float('inf'), 0
num_epochs = 30  # Increase number of epochs

for epoch in range(num_epochs):
    model.train()
    running_loss, correct, total = 0.0, 0, 0
    scaler = torch.cuda.amp.GradScaler()

    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()

        with torch.cuda.amp.autocast():
            outputs = model(images)
            loss = criterion(outputs, labels)

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        running_loss += loss.item()
        total += labels.size(0)
        _, predicted = torch.max(outputs.data, 1)
        correct += (predicted == labels).sum().item()

    avg_loss = running_loss / len(train_loader)
    train_accuracy = 100 * correct / total

    # Validation
    if epoch % 2 == 0:  # Evaluate every 2 epochs
        # Adjusted unpacking to match the return values from evaluate_model
        val_accuracy, val_precision, val_recall, val_f1 = evaluate_model(model, val_loader)

        # Append metrics to lists
        val_accuracies.append(val_accuracy)
        val_precisions.append(val_precision)
        val_recalls.append(val_recall)
        val_f1s.append(val_f1)

        # Save the best model
        if val_accuracy > best_val_loss:
            best_val_loss = val_accuracy
            torch.save({
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': best_val_loss,
            }, checkpoint_path)
            epochs_without_improvement = 0
        else:
            epochs_without_improvement += 1

    if epochs_without_improvement >= patience:
        print("Early stopping...")
        break

print(f"Training completed after {epoch} epochs with best validation accuracy: {best_val_loss:.4f}")

# Evaluate model after training
final_val_accuracy, final_val_precision, final_val_recall, final_val_f1 = evaluate_model(model, val_loader)
print(f"Final Validation Accuracy: {final_val_accuracy:.4f}")
print(f"Final Validation Precision: {final_val_precision:.4f}")
print(f"Final Validation Recall: {final_val_recall:.4f}")
print(f"Final Validation F1 Score: {final_val_f1:.4f}")

"""#### Plotting Metrics over Epochs"""

import matplotlib.pyplot as plt

# Plot Precision, Recall, and F1-Score over epochs
epochs = range(1, len(val_precisions) + 1)

plt.figure(figsize=(12, 4))

# Precision plot
plt.subplot(1, 3, 1)
plt.plot(epochs, val_precisions, label='Validation Precision')
plt.xlabel('Epoch')
plt.ylabel('Precision')
plt.title('Precision over Epochs')
plt.legend()

# Recall plot
plt.subplot(1, 3, 2)
plt.plot(epochs, val_recalls, label='Validation Recall')
plt.xlabel('Epoch')
plt.ylabel('Recall')
plt.title('Recall over Epochs')
plt.legend()

# F1-Score plot
plt.subplot(1, 3, 3)
plt.plot(epochs, val_f1s, label='Validation F1 Score')
plt.xlabel('Epoch')
plt.ylabel('F1 Score')
plt.title('F1 Score over Epochs')
plt.legend()

plt.tight_layout()
plt.show()

"""#### Confusion Matrix"""

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Get predictions and true labels
all_preds = []
all_labels = []

with torch.no_grad():
    for images, labels in val_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        all_preds.extend(predicted.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

# Compute confusion matrix
cm = confusion_matrix(all_labels, all_preds)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)

# Plot confusion matrix
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.show()

"""### Testing"""

from sklearn.metrics import classification_report

# Load test dataset
test_dataset = datasets.ImageFolder(root='/content/drowsiness_data/test', transform=transform)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Evaluate the model on the test set
test_accuracy, test_precision, test_recall, test_f1 = evaluate_model(model, test_loader)

# Print test results
print("\nTest Metrics:")
print(f"Accuracy: {test_accuracy:.4f}%")
print(f"Precision: {test_precision:.4f}")
print(f"Recall: {test_recall:.4f}")
print(f"F1-score: {test_f1:.4f}")

"""#### Confusion Matrix for Testing"""

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Generate confusion matrix
cm = confusion_matrix(all_labels, all_preds)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=test_dataset.classes)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix for Test Data')
plt.show()

"""#### Plottin Metrics for Testing"""

import matplotlib.pyplot as plt

# Test metrics
metrics = ['Precision', 'Recall', 'F1 Score']
values = [test_precision, test_recall, test_f1]

# Plotting the test metrics
plt.figure(figsize=(8, 6))
plt.bar(metrics, values, color=['skyblue', 'salmon', 'lightgreen'])
plt.ylim(0, 1)  # Adjust based on expected range (0 to 1 for normalized scores)
plt.title('Test Metrics')
plt.ylabel('Score')
plt.xlabel('Metric')

# Display plot
plt.show()

"""## Baseline SVM

### Data Transformation, Train/test Split, Feature Extration using Pre-trained CNN Model
"""

import torch
import torchvision.models as models
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score
import numpy as np

# Using the calculated means and standard deviations
mean = [86.20332036, 97.57274173, 128.23810727]
std = [48.52471532, 52.71026118, 58.87592237]

# Define transformations with normalization
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.ToTensor(),
    transforms.Normalize(mean=[m / 255.0 for m in mean], std=[s / 255.0 for s in std])
])

train_data = datasets.ImageFolder(root='/content/drowsiness_data/train', transform=transform)
test_data = datasets.ImageFolder(root='/content/drowsiness_data/test', transform=transform)

train_loader = DataLoader(train_data, batch_size=16, shuffle=True)
test_loader = DataLoader(test_data, batch_size=16, shuffle=False)

from sklearn.model_selection import StratifiedShuffleSplit

# Stratified shuffle split
sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
labels = [label for _, label in train_data]

for train_index, test_index in sss.split(np.zeros(len(labels)), labels):
    train_sampler = torch.utils.data.SubsetRandomSampler(train_index)
    val_sampler = torch.utils.data.SubsetRandomSampler(test_index)

train_loader_stratified = DataLoader(train_data, batch_size=64, sampler=train_sampler)
val_loader_stratified = DataLoader(train_data, batch_size=64, sampler=val_sampler)

# Load a pre-trained CNN model for feature extraction
feature_extractor = models.resnet18(pretrained=True)
feature_extractor.fc = torch.nn.Identity()  # Remove the final classification layer

# Set to evaluation mode
feature_extractor.eval()

# Define image transformations for feature extraction
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# Load the dataset
train_loader=train_loader_stratified
val_loader=val_loader_stratified

"""### Training and Testing Model"""

import torch
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score
import numpy as np

# Check if GPU is available and set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Move the model to the appropriate device
feature_extractor = feature_extractor.to(device)

# Feature extraction function
def extract_features_and_labels(dataloader, model):
    features = []
    labels = []
    model.eval()  # Set model to evaluation mode

    with torch.no_grad():
        for batch in dataloader:
            images, batch_labels = batch
            images = images.to(device)  # Move images to the same device as the model
            batch_features = model(images)  # Extract features

            features.extend(batch_features.cpu().numpy())  # Move to CPU and convert to numpy
            labels.extend(batch_labels.numpy())  # Convert labels to numpy array

    return np.array(features), np.array(labels)

# Extract features from training and testing sets
train_features, train_labels = extract_features_and_labels(train_loader, feature_extractor)
test_features, test_labels = extract_features_and_labels(test_loader, feature_extractor)

# Train the SVM model
svm_model = SVC(kernel='linear', C=1.0, random_state=42)
svm_model.fit(train_features, train_labels)

# Test the model and print metrics
y_pred = svm_model.predict(test_features)
accuracy = accuracy_score(test_labels, y_pred)
precision = precision_score(test_labels, y_pred, average='weighted')
recall = recall_score(test_labels, y_pred, average='weighted')
f1 = f1_score(test_labels, y_pred, average='weighted')

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)
print("\nClassification Report:\n", classification_report(test_labels, y_pred))

"""#### Confusion and Classification Matrix"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Calculate confusion matrix
cm = confusion_matrix(test_labels, y_pred)

# Plotting the metrics
metrics = [accuracy, precision, recall, f1]
metric_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score']

# Visualizing the classification metrics
plt.figure(figsize=(10, 5))
plt.bar(metric_names, metrics, color=['blue', 'orange', 'green', 'red'])
plt.ylim(0, 1)  # Set the y-axis limit to [0, 1]
plt.title('Classification Metrics')
plt.ylabel('Score')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

# Plotting the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Non-Drowsy', 'Drowsy'],
            yticklabels=['Non-Drowsy', 'Drowsy'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""##  SVM with FIS AND NAS

### Data Transformation and Train/Val/Test SetUp
"""

from torch.utils.data import DataLoader, Subset
from torchvision import datasets
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[86.203/255, 97.573/255, 128.238/255], std=[48.525/255, 52.710/255, 58.876/255])
])
train_data = datasets.ImageFolder(root='/content/drowsiness_data/train', transform=transform)
test_data = datasets.ImageFolder(root='/content/drowsiness_data/test', transform=transform)

# Split train data into train/validation
from sklearn.model_selection import StratifiedShuffleSplit
labels = [label for _, label in train_data]
sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)

for train_idx, val_idx in sss.split(np.zeros(len(labels)), labels):
    train_sampler = Subset(train_data, train_idx)
    val_sampler = Subset(train_data, val_idx)

train_loader = DataLoader(train_sampler, batch_size=16, shuffle=True)
val_loader = DataLoader(val_sampler, batch_size=16, shuffle=False)
test_loader = DataLoader(test_data, batch_size=16, shuffle=False)

"""### Feature Extraction"""

# Feature extraction with pre-trained CNN
feature_extractor = models.resnet18(pretrained=True)
feature_extractor.fc = torch.nn.Identity()  # Remove final layer
feature_extractor.eval()

def extract_features(loader):
    features, labels = [], []
    for images, lbls in loader:
        with torch.no_grad():
            feats = feature_extractor(images)
        features.append(feats.numpy())
        labels.extend(lbls.numpy())
    return np.concatenate(features), np.array(labels)

X_train, y_train = extract_features(train_loader)
X_val, y_val = extract_features(val_loader)
X_test, y_test = extract_features(test_loader)

"""### Hyperparameter Tuning"""

from sklearn.model_selection import GridSearchCV

# SVM hyperparameter tuning with GridSearchCV
param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf'],
    'gamma': ['scale', 'auto']
}
grid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Best SVM model from grid search
best_svm = grid_search.best_estimator_

if grid_search.best_estimator_:
    print("Best SVM Model from Grid Search:")
    print(grid_search.best_estimator_)
else:
    print("No model was returned from Grid Search.")

# Optionally print the best parameters
print("Best Parameters:")
print(grid_search.best_params_)

"""### Fuzzy Logic"""

from skfuzzy import control as ctrl

# FIS setup
feature1 = ctrl.Antecedent(np.arange(0, 1, 0.01), 'feature1')
feature2 = ctrl.Antecedent(np.arange(0, 1, 0.01), 'feature2')
output = ctrl.Consequent(np.arange(0, 1, 0.01), 'output')

feature1['low'] = fuzz.trimf(feature1.universe, [0, 0, 0.5])
feature1['high'] = fuzz.trimf(feature1.universe, [0.5, 1, 1])
feature2['low'] = fuzz.trimf(feature2.universe, [0, 0, 0.5])
feature2['high'] = fuzz.trimf(feature2.universe, [0.5, 1, 1])
output['not_drowsy'] = fuzz.trimf(output.universe, [0, 0, 0.5])
output['drowsy'] = fuzz.trimf(output.universe, [0.5, 1, 1])

rule1 = ctrl.Rule(feature1['low'] & feature2['low'], output['not_drowsy'])
rule2 = ctrl.Rule(feature1['high'] | feature2['high'], output['drowsy'])
drowsiness_control = ctrl.ControlSystem([rule1, rule2])
drowsiness_fis = ctrl.ControlSystemSimulation(drowsiness_control)

# SVM model with FIS adjustment
svm = SVC(C=10, kernel='rbf', gamma='scale')
svm.fit(X_train, y_train)

"""#### Fuzzy Logic Output"""

for i in range(10):  # Check the first 10 samples
    drowsiness_fis.input['feature1'] = X_val[i][0]
    drowsiness_fis.input['feature2'] = X_val[i][1]
    drowsiness_fis.compute()
    print(f'FIS Output for Sample {i}: {drowsiness_fis.output["output"]}')

"""### Training and Validation Loop"""

# Tracking metrics
train_accuracies, train_precisions, train_recalls, train_f1_scores = [], [], [], []
val_accuracies, val_precisions, val_recalls, val_f1_scores = [], [], [], []

# Training loop with metrics calculation
for epoch in range(10):
    # Training phase
    train_predictions = []
    for i in range(len(X_train)):
        drowsiness_fis.input['feature1'] = X_train[i][0]
        drowsiness_fis.input['feature2'] = X_train[i][1]
        drowsiness_fis.compute()
        fis_output = drowsiness_fis.output['output']

        svm_pred = svm.predict([X_train[i]])
        if fis_output < 0.5:
            svm_pred[0] = 1
        train_predictions.append(svm_pred[0])

    # Calculate and store training metrics
    train_accuracy = accuracy_score(y_train, train_predictions)
    train_precision = precision_score(y_train, train_predictions)
    train_recall = recall_score(y_train, train_predictions)
    train_f1 = f1_score(y_train, train_predictions)

    train_accuracies.append(train_accuracy)
    train_precisions.append(train_precision)
    train_recalls.append(train_recall)
    train_f1_scores.append(train_f1)

    # Validation phase
    val_predictions = []
    for i in range(len(X_val)):
        drowsiness_fis.input['feature1'] = X_val[i][0]
        drowsiness_fis.input['feature2'] = X_val[i][1]
        drowsiness_fis.compute()
        fis_output = drowsiness_fis.output['output']

        svm_pred = svm.predict([X_val[i]])
        if fis_output < 0.5:
            svm_pred[0] = 1
        val_predictions.append(svm_pred[0])

    # Calculate and store validation metrics
    val_accuracy = accuracy_score(y_val, val_predictions)
    val_precision = precision_score(y_val, val_predictions)
    val_recall = recall_score(y_val, val_predictions)
    val_f1 = f1_score(y_val, val_predictions)

    val_accuracies.append(val_accuracy)
    val_precisions.append(val_precision)
    val_recalls.append(val_recall)
    val_f1_scores.append(val_f1)

# Plot metrics over epochs
epochs = list(range(1, 11))
plt.plot(epochs, train_accuracies, label='Train Accuracy')
plt.plot(epochs, val_accuracies, label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training vs Validation Accuracy Over Epochs')
plt.legend()
plt.show()

svm_predictions = best_svm.predict(X_val)

accuracy = accuracy_score(y_val, svm_predictions)
precision = precision_score(y_val, svm_predictions)
recall = recall_score(y_val, svm_predictions)
f1 = f1_score(y_val, svm_predictions)

print(f'SVM Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}')

from sklearn.metrics import confusion_matrix
import seaborn as sns

confusion_mat = confusion_matrix(y_val, predictions)
sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

"""### Testing Loop"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt

# Assuming you have your test data as X_test and y_test
# Predictions on the test set
test_predictions = []
for i in range(len(X_test)):
    drowsiness_fis.input['feature1'] = X_test[i][0]
    drowsiness_fis.input['feature2'] = X_test[i][1]
    drowsiness_fis.compute()
    fis_output = drowsiness_fis.output['output']

    svm_pred = svm.predict([X_test[i]])
    if fis_output < 0.5:
        svm_pred[0] = 1  # Adjust SVM prediction with FIS output
    test_predictions.append(svm_pred[0])

# Calculate metrics for the test set
test_accuracy = accuracy_score(y_test, test_predictions)
test_precision = precision_score(y_test, test_predictions)
test_recall = recall_score(y_test, test_predictions)
test_f1 = f1_score(y_test, test_predictions)

# Print the test metrics
print("Test Accuracy:", test_accuracy)
print("Test Precision:", test_precision)
print("Test Recall:", test_recall)
print("Test F1 Score:", test_f1)

# Plotting metrics for the test set
metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']
values = [test_accuracy, test_precision, test_recall, test_f1]

plt.figure(figsize=(10, 6))
plt.bar(metrics, values, color=['blue', 'orange', 'green', 'red'])
plt.ylim(0, 1)  # Set y-axis limit from 0 to 1
plt.title('Test Metrics')
plt.ylabel('Score')
plt.xlabel('Metrics')
plt.grid(axis='y')

# Annotate bars with their values
for i in range(len(metrics)):
    plt.text(i, values[i] + 0.02, f"{values[i]:.2f}", ha='center')

plt.show()

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Calculate confusion matrix
cm = confusion_matrix(y_test, test_predictions)

# Display confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Not Drowsy', 'Drowsy'])
disp.plot(cmap=plt.cm.Blues)  # Change the colormap to your preference
plt.title('Confusion Matrix for Test Dataset')
plt.show()

